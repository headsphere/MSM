---
title: "Modern Statistical Methods Assignment"
author: "Nick Head"
date: "15 April 2015"
output: pdf_document
header-includes: \usepackage[]{amsmath}
---

```{r echo=FALSE}
library(knitr)
read_chunk("RandomPermutationTest.R")
read_chunk("AcceptReject.r")
read_chunk("BootstrapTTest.r")
```

```{r RandomPermutationTest, echo=FALSE}
```
```{r SimulateAcceptReject, echo=FALSE}
```
```{r BootstrapTTest, echo=FALSE}
```


##Question 1

\[
 f(x) =
  \begin{cases}
   6 x (1 - x)^2        & \text{if } 0 < x < 1 \\
   6 (2 - x) (1 - x)^2  & \text{if } 1 \leq x < 2 \\
   0                    & \text{otherwise}
  \end{cases}
\]

###a)

```{r}
fn <- function(x) 
  {
    ifelse( x > 0 & x < 1, 6*x*(1 - x)^2, 
            ifelse( x >= 1 & x < 2, 6*(2-x)*(1 - x)^2, 
                    0))
  }

curve(fn, from = -1, to = 3)
```

\newpage

###b)

See Appendix 1 for further implemention details of the Accept-Reject algorithm.

```{r cache=TRUE}
Nsim = 10000
set.seed(1234)
x = SimulateAcceptReject(Nsim, 
                         f_fn = fn,
                         g_fn = function (y) {dunif(y, min = 0, max = 2)},
                         randg_fn = function (Nsim) {runif(Nsim, min = 0, max = 2)},
                         x_interval = c(0, 2))

```

###c)

i) $$ E[log(X)] = `r mean(log(x))` $$ 

ii) $$ P(log(X) < 0.5) = `r sum(log(x) < 0.5) / length(x)` $$  

iii) $$ E[(log(X) | X > 0.5] = `r mean(log(x[x > 0.5]))`$$ 

\newpage

###d)

```{r echo=FALSE}
s = c(0.308, 1.936, 1.311, 0.536, 1.606, 1.514, 1.630, 1.295, 0.811, 1.145, 1.429, 0.811, 0.796, 0.988, 1.969, 1.161, 0.271, 0.053, 0.629, 1.171, 0.056, 1.313, 0.592, 0.934,  0.723)

plot(ecdf(x), main="Theoretical CDF of Generated X vs Empirical CDF of S", col="red")
plot(ecdf(s), add=TRUE, ylab = FALSE, xlab=FALSE, verticals = TRUE, do.points = FALSE, col = "blue")
```

\newpage

###e)

```{r results = 'asis', echo=FALSE}
library(xtable)
print(xtable(summary(data.frame(x=x, s=s))), comment=FALSE)
boxplot(x, s, names = c("x", "s"))
```

As can be seen from the summary statistics and boxplots, the means of both $x$ and $s$ appear to be almost identical, as are the minimum and maximum values, indicating that both distributions appear to have similar supports. The main difference appears to be in the size of the interquartile ranges and hence the variances. $x$ appears to have a much larger variance than $s$ although both do appear to be somewhat symmetric around the mean.

```{r echo=FALSE}
par(mfrow=c(1, 2))
hist(x)
hist(s)
```

The histograms however appear to tell a different story. While $x$ clearly appears bimodal, $s$ clearly only appears to have one mode.

###f)

```{r warning=FALSE}
ks = ks.test(x, s)
ks
```

The Kolmogorov-Smirnoff tested conducted above indicates that we cannot reject the null hypothesis that the two datasets come from different distributions (at the 5% level, with a p-value of `r ks$p.value`). The statistic $D_n$ is calculated as the largest absolute deviation between the empirical distribution and the theoretical one against which we are comparing it. For this run the $D_n$ statistic is calculated as `r ks$statistic`. This can be seen visually in the chart in section d) as roughly the largest deviation between the theoretical CDF of $x$ and the empirical CDF of $s$.  

This does however contradict some of our earlier findings. Even though some of the statistics such as mean and support of the distributions do indeed appear to be similar, by looking at the histograms the bimodal nature of the Camel distribution is clearly missing in the $s$ distribution. Without this key feature of the empirical dataset, we should be quite cynical of the results of this non-parametric test.

\newpage

##Question 2

###a)

```{r}
twain = c(0.225, 0.262, 0.217, 0.240, 0.230, 0.229, 0.235, 0.217)

snod = c(0.209, 0.205, 0.196, 0.210, 0.202, 0.207, 0.224, 0.223, 0.220, 0.201)

ttest = t.test(twain, snod)
```

The t-test gives a p-value of: `r ttest$p.value`

###b)

The following test does a randomized permutation. The code is detailed in Appendix 2 
```{r tidy=TRUE, cache=TRUE}
pvalue = RandomPermutationTest(twain, snod)
```
The random permutation test gives a p-value of: `r pvalue`

This can be compared with an exact randomization test:
```{r warning=FALSE, message=FALSE, cache=TRUE}
library(exactRankTests)
exact = perm.test(x = twain, y = snod, exact = TRUE)
```
The exact randomization test gives a p-value of: `r exact$p.value`

###c)

The following test uses a bootstrap approach. The code is detailed in Appendix 3 

```{r cache=TRUE}
boot.p = BootstrapTTest(twain, snod)
```
The bootstrapping approach gives us a p-value of: `r boot.p`

###d)

Using all three tests (t-test, permutation and bootstrapped t-test) the p-values are between 0.003 and 0.001, therefore giving very strong support (at the $\alpha = 0.01$ level) to reject the null hypothesis of equality of means. Based off this we are therefore led to believe that the two sets of letters are indeed written by two different people. 

Even though the three sets of tests all lead to the same conclusions we must still be wary of whether the tests are appropriate for this analysis. With a standard t-test although it is robust to non-normality, it is still worth checking if the data is somewhat normal considering we are dealing with proportions which generally would be definitely non-normal. For this reason we can conduct a QQ test of the Twain and Snodgrass datasets:

```{r echo=FALSE, fig.align='center'}
par(mfrow=c(1, 2))
qqnorm(twain, main = "Normal Q-Q Plot: Twain")
qqline(twain)
qqnorm(snod, main = "Normal Q-Q Plot: Snodgrass")
qqline(snod)
```

The QQ tests do not indicate any significant departures from normality therefore we have no reason to distrust the conclusions from the t-test. If however the data had exhibited signs of non-normality, only the results of the bootstrap test would have been trustworthy. 

A further reason to potentially distrust the t-test would be due to the small sample sizes. For this reason we are satisfied that both the permutation and bootstrap tests also give us the same asymptotic results.

\newpage

## Appendix 1: Accept-Reject Code Listing

```{r SimulateAcceptReject, tidy=TRUE}
```

\newpage

## Appendix 2: Random Permutation Test Code Listing

```{r RandomPermutationTest, tidy=TRUE}
```

\newpage

## Appendix 3: Bootstrap T Test Code Listing

```{r BootstrapTTest, tidy=TRUE}
```


